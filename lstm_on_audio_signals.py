# -*- coding: utf-8 -*-
"""LSTM on Audio Signals

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/amineipad/lstm-on-audio-signals.a268ea40-868d-4dc8-9730-1fb09956666f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250421/auto/storage/goog4_request%26X-Goog-Date%3D20250421T143027Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da819d13237c1637d30cb9744362aab86b3a54af81b1882fc63a86d3b6960aab45c908bc2dec0ca81c17dfe3b26f72466ea490cd783ed3ea9c95c1e40b85b01f824656df6447169782326fccc04e7704567afb9228ad83bc2be13d5c97f15f5f0472498885b55590b095e4801e72c06eb1a6b4208fe2d9e44275de2be26ad97792f3c33b8a4f6d8f35900f21c7a2b49cb3e03c2b574098d257155e1bc3043ba9d6d1a7f90ef015ee1a8549e7d135a26a6c3c8be8e1399c683d9a239681346235ca633939c1fa46a028a00a8cb0618e17ba41ab69267f73102eec3e5ca0b5ee0cef0d5a978fc98389254de2af02a4586c973efd483f593947eff6d3834adfd0a70
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
julienjta_engine_acoustic_emissions_path = kagglehub.dataset_download('julienjta/engine-acoustic-emissions')

print('Data source import complete.')

"""# Installing *torch-summary*"""

!pip install torch-summary

"""# Importing necessary libraries"""

import scipy
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
import torch.nn.functional as F
from torchsummary import summary
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

"""# Checking if GPU is available"""

if torch.cuda.is_available():
  device = torch.device("cuda:0")
  print("GPU Runtime Detected")

else:
  device = torch.device("cpu")
  print("No GPU Found - CPU Runtime")

"""# Defining the *splitter* function"""

def splitter(array, win_len, hop_len, return_df = True):
   N = array.shape[0]
   m = 0
   ids = []
   while m + win_len < N:
      ids.append([m, m + win_len])
      m = m + hop_len

   if return_df:
      return pd.DataFrame([array[i[0]: i[1]] for i in ids])
   else:
      return np.array([array[i[0]: i[1]] for i in ids])

"""# Loading the Dataset"""

file = "../input/engine-acoustic-emissions/dataset.mat"
dataset = scipy.io.loadmat(file)
dataset.keys()

"""# Preprocessing

## Splitting the original Dataset
"""

win_len, hop_len = 8000, 1000
df_list = []

for state in ['normal', 'inner', 'roller', 'outer']:
    temp_df = splitter(dataset[state].reshape((-1,)), win_len, hop_len, return_df = True)
    temp_df['state'] = state
    df_list.append(temp_df)

df = pd.concat(df_list).reset_index(drop = True)
df

"""## Train/Test Splitting"""

df_train, df_test = train_test_split(df, test_size = 0.3, random_state = 42)

"""## Feature/Target Declaration"""

x_train, x_test = df_train.iloc[:, :-1], df_test.iloc[:, :-1]
y_train, y_test = df_train.iloc[:, -1], df_test.iloc[:, -1]

"""## Sequencing Time-Series of *x_train* and *x_test*"""

x_train_seq = []
for index, row in x_train.iterrows():
    x_train_seq.append(splitter(row, 1000, 1000, return_df = False))

x_train_seq = np.array(x_train_seq)
x_train_seq.shape

x_test_seq = []
for index, row in x_test.iterrows():
    x_test_seq.append(splitter(row, 1000, 1000, return_df = False))

x_test_seq = np.array(x_test_seq)
x_test_seq.shape

"""## Feature Scaling"""

for i in range(x_train_seq.shape[0]):
    for j in range(x_train_seq.shape[1]):
        x_train_seq[i, j, :] = (x_train_seq[i, j, :] - x_train_seq[i, j, :].mean()) / x_train_seq[i, j, :].std()

x_train_seq.shape

for i in range(x_test_seq.shape[0]):
    for j in range(x_test_seq.shape[1]):
        x_test_seq[i, j, :] = (x_test_seq[i, j, :] - x_test_seq[i, j, :].mean()) / x_test_seq[i, j, :].std()

x_test_seq.shape

"""## Target Encoding"""

labelencoder= LabelEncoder()
y_train_encoded = labelencoder.fit_transform(y_train.to_numpy())
y_test_encoded = labelencoder.transform(y_test.to_numpy())

"""# Training

## Model Declaration
"""

lstm = torch.nn.LSTM(1000, 500, batch_first = True)
fc1 = torch.nn.Linear(500, 100)
fc2 = torch.nn.Linear(100, 4)

lstm_out, (h_n, c_n) = lstm(torch.tensor(x_train_seq).float())
lstm_out[:, -1, :].size()

class Classifier(torch.nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()

        self.lstm = torch.nn.LSTM(input_size = 1000, hidden_size = 500, batch_first = True)
        self.fc1 = torch.nn.Linear(500, 100)
        self.fc2 = torch.nn.Linear(100, 4)

    def forward(self, x):

        lstm_out, (h_n, c_n) = self.lstm(x)
        z = lstm_out[:, -1, :]
        z = z.view(-1, 500)
        z = torch.tanh(self.fc1(z))
        z = self.fc2(z)

        return z

"""## Fitting the Model"""

x_train_train, x_train_validation, y_train_train, y_train_validation = train_test_split(x_train_seq, y_train_encoded, test_size = 0.25)

x_train_train = x_train_train.reshape(-1, 7, 1000)
x_train_validation = x_train_validation.reshape(-1, 7, 1000)

x_train_VAR = torch.autograd.Variable(torch.Tensor(x_train_train).float()).to(device)
y_train_VAR = torch.autograd.Variable(torch.LongTensor(y_train_train)).to(device)
x_valid_VAR = torch.autograd.Variable(torch.Tensor(x_train_validation).float()).to(device)
y_valid_VAR = torch.autograd.Variable(torch.LongTensor(y_train_validation)).to(device)

lr = 0.001
ep = 50

model = Classifier().to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),
                             lr = lr,
                             weight_decay = lr / ep)

losses = []
valid_losses = []
accs = []
valid_accs = []



for epoch in range(ep):

  # validation step
  valid_loss = criterion(model(x_valid_VAR), y_valid_VAR).item()
  valid_losses.append(valid_loss)
  valid_acc = accuracy_score(y_train_validation, np.argmax(model(x_valid_VAR).cpu().detach().numpy(), axis = 1))
  valid_accs.append(valid_acc)

  # training step
  optimizer.zero_grad()
  loss = criterion(model(x_train_VAR), y_train_VAR)
  acc = accuracy_score(y_train_train, np.argmax(model(x_train_VAR).cpu().detach().numpy(), axis = 1))
  accs.append(acc)
  losses.append(loss.item())
  loss.backward()
  optimizer.step()
  print(f"Epoch {epoch+1}, loss: {np.round(loss.item(), 4)}  , Vloss: {np.round(valid_loss, 4)}, acc: {np.round(acc, 4)}, Vacc: {np.round(valid_acc, 4)}")

fig, axes = plt.subplots(1, 2, figsize=(16,4))
fig.suptitle('Deep Learning Model Training Process')
axes[0].plot(losses, label='Training Loss')
axes[0].plot(valid_losses, label='Validation Loss')
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss Vs. Epochs')
axes[0].legend()

axes[1].plot(accs, label='Training Accuracy')
axes[1].plot(valid_accs, label='Validation Accuracy')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Accuracy Vs. Epochs')
axes[1].legend()
plt.show()

"""# Evaluation

## Test-set Accuracy
"""

x_test_VAR = torch.autograd.Variable(torch.Tensor(x_test_seq.reshape(-1, 7, 1000)).float()).to(device)
testing_acc = accuracy_score(y_test_encoded, np.argmax(F.softmax(model(x_test_VAR), dim = 0).cpu().detach().numpy(), axis = 1))
print('Held-out Test-set Accuracy: ', round(testing_acc, 4))

"""## Test-set Confusion Matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_test_pred = np.argmax(F.softmax(model(x_test_VAR), dim = 0).cpu().detach().numpy(), axis = 1)
y_test_pred_decoded = labelencoder.inverse_transform(y_test_pred)

matrix = confusion_matrix(y_test_encoded, y_test_pred)

ax = sns.heatmap(matrix, annot=True, fmt='d', cbar = True, square = True, cmap = 'Blues')
ax.set_xlabel("Predicted", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(np.unique(y_test_pred_decoded))
ax.set_ylabel("Actual", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(np.unique(y_test_pred_decoded))
ax.set_title("Confusion Matrix", fontsize=14, pad=20)
plt.show()

